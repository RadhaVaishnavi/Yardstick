{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KLWvVOZv5xr"
      },
      "source": [
        "This notebook implements two methods for semantic search:\n",
        "\n",
        "**OpenAI API**:\n",
        "Uses OpenAI's embedding generation to calculate semantic similarity. This approach offers high accuracy but is subject to usage limits (API key restrictions and quota).\n",
        "\n",
        "**Sentence Transformers**:\n",
        "Utilizes the sentence-transformers library to compute embeddings locally. This is a cost-effective alternative that works seamlessly without external API dependencies, making it suitable for unrestricted or large-scale usage.\n",
        "\n",
        "Since the OpenAI API usage limit has been reached, Sentence Transformers is used in this notebook for further processing. Pre-trained models such as all-MiniLM-L6-v2 are leveraged to generate high-quality embeddings for caption retrieval.\n",
        "\n",
        "Both methods enable ranking and retrieval of captions based on semantic similarity, and the implementation ensures consistency between approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSPvb97nMeaZ"
      },
      "source": [
        "**Sentence Transformers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccg28-41MdVG",
        "outputId": "c968ca47-d2a9-4db8-a49e-14b415eea4b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
            "Requirement already satisfied: langchain-pinecone in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.12.14)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.29)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.27.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain) (3.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.2)\n",
            "Created new index: business-qa-index\n",
            "Enter your question (or type 'exit' to quit): how to troubleshoot?\n",
            "Question: how to troubleshoot?\n",
            "Context:\n",
            "## Troubleshooting\n",
            "If you encounter issues, try the following:\n",
            "- Issue: Device not turning on.\n",
            "Solution: Ensure the power cable is securely connected.\n",
            "- Issue: Cloud sync not working.\n",
            "Solution: Check your internet connection and restart the device.\n",
            "\n",
            "Enter your question (or type 'exit' to quit): exit\n",
            "Exiting...\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install \\\n",
        "    \"pinecone-client\" \\\n",
        "    \"langchain\" \\\n",
        "    \"langchain-text-splitters\" \\\n",
        "    \"sentence-transformers\" \\\n",
        "    \"langchain-pinecone\"\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"replace your key\"\n",
        "\n",
        "# Load business data\n",
        "business_document = \"\"\"\n",
        "# Company XYZ Product Manual\n",
        "\n",
        "## Introduction\n",
        "Welcome to Company XYZ! This document provides an overview of our flagship product, the XYZ3000, and its features.\n",
        "\n",
        "## Product Overview\n",
        "The XYZ3000 is a state-of-the-art device designed to streamline your workflow. Key features include:\n",
        "- Advanced AI integration for predictive analytics.\n",
        "- Cloud-based synchronization for seamless data access.\n",
        "- Intuitive user interface for ease of use.\n",
        "\n",
        "## Getting Started\n",
        "To set up the XYZ3000, follow these steps:\n",
        "1. Unbox the device and ensure all components are present.\n",
        "2. Connect the device to a power source and turn it on.\n",
        "3. Follow the on-screen instructions to complete the setup.\n",
        "\n",
        "## Troubleshooting\n",
        "If you encounter issues, try the following:\n",
        "- Issue: Device not turning on.\n",
        "  Solution: Ensure the power cable is securely connected.\n",
        "- Issue: Cloud sync not working.\n",
        "  Solution: Check your internet connection and restart the device.\n",
        "\"\"\"\n",
        "\n",
        "# Split the document into chunks\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "headers_to_split_on = [(\"##\", \"Header 2\")]\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "document_chunks = markdown_splitter.split_text(business_document)\n",
        "\n",
        "# Initialize Sentence Transformers embeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Initialize Pinecone and delete existing index (if any)\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import time\n",
        "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "index_name = \"business-qa-index\"\n",
        "if index_name in pc.list_indexes().names():\n",
        "    pc.delete_index(index_name)\n",
        "    print(f\"Deleted index: {index_name}\")\n",
        "\n",
        "# Create a new Pinecone index with 384 dimensions\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,  # Sentence Transformers embeddings have 384 dimensions\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "    print(f\"Created new index: {index_name}\")\n",
        "\n",
        "# Wait for the index to be ready\n",
        "while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)\n",
        "\n",
        "# Embed and upsert chunks\n",
        "document_embeddings = embedding_model.encode([chunk.page_content for chunk in document_chunks])\n",
        "index = pc.Index(index_name)\n",
        "namespace = \"business-docs\"\n",
        "for i, (chunk, embedding) in enumerate(zip(document_chunks, document_embeddings)):\n",
        "    index.upsert(\n",
        "        vectors=[\n",
        "            {\n",
        "                \"id\": f\"chunk-{i}\",\n",
        "                \"values\": embedding,\n",
        "                \"metadata\": {\"text\": chunk.page_content}\n",
        "            }\n",
        "        ],\n",
        "        namespace=namespace\n",
        "    )\n",
        "time.sleep(5)\n",
        "\n",
        "# Initialize the retriever\n",
        "from langchain_pinecone import PineconeVectorStore  # Updated import\n",
        "\n",
        "# Define a custom Embeddings class for Sentence Transformers\n",
        "from langchain_core.embeddings import Embeddings\n",
        "\n",
        "class SentenceTransformerEmbeddings(Embeddings):\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.model.encode(texts).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.model.encode(text).tolist()\n",
        "\n",
        "# Initialize the custom embeddings\n",
        "embeddings = SentenceTransformerEmbeddings(embedding_model)\n",
        "\n",
        "# Initialize the vector store\n",
        "vectorstore = PineconeVectorStore(index, embeddings, \"text\", namespace=namespace)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})  # Retrieve only the top 1 result\n",
        "\n",
        "# Define a simple QA function\n",
        "def ask_question(query):\n",
        "    # Generate query embedding\n",
        "    query_embedding = embeddings.embed_query(query)\n",
        "\n",
        "    # Query Pinecone\n",
        "    query_response = index.query(\n",
        "        vector=query_embedding,\n",
        "        top_k=1,  # Retrieve only the top 1 result\n",
        "        include_metadata=True,\n",
        "        namespace=namespace\n",
        "    )\n",
        "\n",
        "    # Combine the retrieved documents into a single context\n",
        "    if query_response[\"matches\"]:\n",
        "        context = \"\\n\\n\".join([match[\"metadata\"][\"text\"] for match in query_response[\"matches\"]])\n",
        "    else:\n",
        "        context = \"No relevant documents found.\"\n",
        "\n",
        "    # Print the question and context\n",
        "    print(f\"Question: {query}\")\n",
        "    print(f\"Context:\\n{context}\\n\")\n",
        "\n",
        "    # Return the context (you can replace this with a custom LLM if needed)\n",
        "    return context\n",
        "\n",
        "# Interactive loop for user input\n",
        "while True:\n",
        "    # Get user input\n",
        "    query = input(\"Enter your question (or type 'exit' to quit): \")\n",
        "\n",
        "    # Exit the loop if the user types 'exit'\n",
        "    if query.lower() == \"exit\":\n",
        "        print(\"Exiting...\")\n",
        "        break\n",
        "\n",
        "    # Get and display the answer\n",
        "    ask_question(query)\n",
        "\n",
        "# Clean up\n",
        "pc.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qPOih4iMjiP"
      },
      "source": [
        "**OpenAI API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh4uq5cFMWrR"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install \\\n",
        "    \"pinecone-client\" \\\n",
        "    \"langchain\" \\\n",
        "    \"langchain-openai\" \\\n",
        "    \"langchain-text-splitters\"\n",
        "\n",
        "# Set environment variables\n",
        "import os\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"your key\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your key\"\n",
        "\n",
        "# Load business data\n",
        "business_document = \"\"\"\n",
        "# Company XYZ Product Manual\n",
        "\n",
        "## Introduction\n",
        "Welcome to Company XYZ! This document provides an overview of our flagship product, the XYZ3000, and its features.\n",
        "\n",
        "## Product Overview\n",
        "The XYZ3000 is a state-of-the-art device designed to streamline your workflow. Key features include:\n",
        "- Advanced AI integration for predictive analytics.\n",
        "- Cloud-based synchronization for seamless data access.\n",
        "- Intuitive user interface for ease of use.\n",
        "\n",
        "## Getting Started\n",
        "To set up the XYZ3000, follow these steps:\n",
        "1. Unbox the device and ensure all components are present.\n",
        "2. Connect the device to a power source and turn it on.\n",
        "3. Follow the on-screen instructions to complete the setup.\n",
        "\n",
        "## Troubleshooting\n",
        "If you encounter issues, try the following:\n",
        "- Issue: Device not turning on.\n",
        "  Solution: Ensure the power cable is securely connected.\n",
        "- Issue: Cloud sync not working.\n",
        "  Solution: Check your internet connection and restart the device.\n",
        "\"\"\"\n",
        "\n",
        "# Split the document into chunks\n",
        "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
        "headers_to_split_on = [(\"##\", \"Header 2\")]\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
        "document_chunks = markdown_splitter.split_text(business_document)\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Create a Pinecone index\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import time\n",
        "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
        "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
        "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
        "spec = ServerlessSpec(cloud=cloud, region=region)\n",
        "index_name = \"business-qa-index\"\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,  # OpenAI embeddings have 1536 dimensions\n",
        "        metric=\"cosine\",\n",
        "        spec=spec\n",
        "    )\n",
        "    while not pc.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)\n",
        "\n",
        "# Embed and upsert chunks\n",
        "document_embeddings = embeddings.embed_documents([chunk.page_content for chunk in document_chunks])\n",
        "index = pc.Index(index_name)\n",
        "namespace = \"business-docs\"\n",
        "for i, (chunk, embedding) in enumerate(zip(document_chunks, document_embeddings)):\n",
        "    index.upsert(\n",
        "        vectors=[\n",
        "            {\n",
        "                \"id\": f\"chunk-{i}\",\n",
        "                \"values\": embedding,\n",
        "                \"metadata\": {\"text\": chunk.page_content}\n",
        "            }\n",
        "        ],\n",
        "        namespace=namespace\n",
        "    )\n",
        "time.sleep(5)\n",
        "\n",
        "# Initialize the chatbot\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain import hub\n",
        "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
        "from langchain.vectorstores import Pinecone\n",
        "vectorstore = Pinecone(index, embeddings, \"text\", namespace=namespace)\n",
        "retriever = vectorstore.as_retriever()\n",
        "llm = ChatOpenAI(openai_api_key=os.environ.get('OPENAI_API_KEY'), model_name='gpt-4', temperature=0.0)\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
        "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "\n",
        "# Ask questions\n",
        "query1 = \"What are the key features of the XYZ3000?\"\n",
        "query2 = \"How do I troubleshoot if the device is not turning on?\"\n",
        "answer1 = retrieval_chain.invoke({\"input\": query1})\n",
        "print(\"Query 1:\", query1)\n",
        "print(\"\\nAnswer with knowledge:\\n\\n\", answer1['answer'])\n",
        "print(\"\\nContext used:\\n\\n\", answer1['context'])\n",
        "answer2 = retrieval_chain.invoke({\"input\": query2})\n",
        "print(\"\\nQuery 2:\", query2)\n",
        "print(\"\\nAnswer with knowledge:\\n\\n\", answer2['answer'])\n",
        "print(\"\\nContext used:\\n\\n\", answer2['context'])\n",
        "\n",
        "# Clean up\n",
        "pc.delete_index(index_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
